{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:2263: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./saved_models/cached_models/gpt2\n",
      "distilgpt2\n",
      "./saved_models/cached_models/distilgpt2\n",
      "EleutherAI/gpt-neo-125M\n",
      "./saved_models/cached_models/EleutherAI/gpt-neo-125M\n",
      "Qwen/Qwen3-1.7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./saved_models/cached_models/Qwen/Qwen3-1.7B\n"
     ]
    }
   ],
   "source": [
    "# model download \n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Download the models from huggingface and save them in the cached_models folder\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for model_name in [\"gpt2\",\"distilgpt2\",\"EleutherAI/gpt-neo-125M\",\"Qwen/Qwen3-1.7B\"]:\n",
    "    print(model_name)\n",
    "    if model_name in [\"gpt2\", \"distilgpt2\"]:\n",
    "        model = AutoModelWithLMHead.from_pretrained(model_name).to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")   # Initialize tokenizer\n",
    "        # number of heads per layer, and number of layers\n",
    "        num_heads, num_layers = model.config.n_head, model.config.n_layer\n",
    "        head_dim, max_length = int(model.config.n_embd/num_heads), model.config.n_positions\n",
    "\n",
    "    elif model_name in [\"EleutherAI/gpt-neo-125M\"]:\n",
    "        model = GPTNeoForCausalLM.from_pretrained(model_name).to(device)\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "        num_heads, num_layers = model.config.num_heads, model.config.num_layers\n",
    "        head_dim, max_length = int(model.config.hidden_size/num_heads), model.config.max_position_embeddings\n",
    "\n",
    "    elif model_name in [\"Qwen/Qwen3-1.7B\"]:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-1.7B\").to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\", padding_side='left')\n",
    "        num_heads, num_layers = model.config.num_attention_heads, model.config.num_hidden_layers\n",
    "        head_dim, max_length = int(model.config.hidden_size/num_heads), model.config.max_position_embeddings\n",
    "\n",
    "    model.save_pretrained(\"./saved_models/cached_models/\" + model_name)\n",
    "    tokenizer.save_pretrained(\"./saved_models/cached_tokenizers/\" + model_name)\n",
    "    print(\"./saved_models/cached_models/\" + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 85.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# create model_config.json \n",
    "\n",
    "import json \n",
    "import torch\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_names = [\"gpt2\",\"distilgpt2\",\"EleutherAI/gpt-neo-125M\",\"Qwen/Qwen3-1.7B\"]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "models_config = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "\n",
    "    models_config[model_name] = {}\n",
    "\n",
    "    if model_name in [\"gpt2\", \"distilgpt2\"]:\n",
    "        model = AutoModelWithLMHead.from_pretrained(\"./saved_models/cached_models/\" + model_name).to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"./saved_models/cached_tokenizers/\" + model_name, padding_side=\"left\")    \n",
    "        num_heads, num_layers = model.config.n_head, model.config.n_layer\n",
    "        head_dim, max_length = int(model.config.n_embd/num_heads), model.config.n_positions\n",
    "\n",
    "    elif model_name in [\"EleutherAI/gpt-neo-125M\"]:\n",
    "        model = GPTNeoForCausalLM.from_pretrained(\"./saved_models/cached_models/\" + model_name).to(device)\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"./saved_models/cached_tokenizers/\" + model_name, padding_side=\"left\")\n",
    "        num_heads, num_layers = model.config.num_heads, model.config.num_layers\n",
    "        head_dim, max_length = int(model.config.hidden_size/num_heads), model.config.max_position_embeddings\n",
    "\n",
    "    elif model_name in [\"Qwen/Qwen3-1.7B\"]:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"./saved_models/cached_models/\" + model_name).to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"./saved_models/cached_tokenizers/\" + model_name, padding_side=\"left\")\n",
    "        num_heads, num_layers = model.config.num_attention_heads, model.config.num_hidden_layers\n",
    "        head_dim, max_length = int(model.config.hidden_size/num_heads), model.config.max_position_embeddings\n",
    "\n",
    "    models_config[model_name][\"num_heads\"] = num_heads\n",
    "    models_config[model_name][\"num_layers\"] = num_layers\n",
    "    models_config[model_name][\"head_dim\"] = head_dim\n",
    "    models_config[model_name][\"max_length\"] = max_length\n",
    "\n",
    "json.dump(models_config, open(\"./models_config.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create scripts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "id = 0\n",
    "folder_name = \"experiment_1\"\n",
    "heads_per_sh = 10\n",
    "for group in [\"gender_and_sex\"]:\n",
    "  for model in [\"EleutherAI/gpt-neo-125M\"]:\n",
    "\n",
    "      if model == \"EleutherAI/gpt-neo-125M\":\n",
    "        num_all_heads = 144 \n",
    "\n",
    "      for prompting in [\"holistic\"]:\n",
    "        for head_knockout in range(0,num_all_heads,heads_per_sh):\n",
    "          for seed in range(1,4):\n",
    "            my_file = open(\"./scripts/sample.sh\")\n",
    "            string_list = my_file.readlines()\n",
    "\n",
    "            my_file.close()\n",
    "            string_list[1] = \"#SBATCH --account=def-bengioy_cpu\" + \"\\n\"\n",
    "            string_list[2] = \"#SBATCH --cpus-per-task=1\" + \"\\n\"\n",
    "            string_list[3] =  \"\\n\"\n",
    "            string_list[4] = \"#SBATCH --mem=25G\" + \"\\n\"\n",
    "            string_list[8] = \"python main.py \"\n",
    "\n",
    "            if model == \"EleutherAI/gpt-neo-125M\":\n",
    "              model_name = \"N1\"\n",
    "              string_list[4] = \"#SBATCH --mem=50G\" + \"\\n\"\n",
    "\n",
    "            elif model == \"EleutherAI/gpt-j-6B\":\n",
    "              model_name = \"NJ\"\n",
    "              string_list[1] = \"#SBATCH --account=def-bengioy_gpu\" + \"\\n\"\n",
    "              string_list[3] = \"#SBATCH --gres=gpu:1\" + \"\\n\"\n",
    "              string_list[4] = \"#SBATCH --mem=100G\" + \"\\n\"\n",
    "              string_list[8] += \" --batch_size 64 \"\n",
    "\n",
    "            elif model == \"meta-llama/Llama-2-7b-chat-hf\":\n",
    "              model_name = \"L7\"\n",
    "              string_list[1] = \"#SBATCH --account=def-bengioy_gpu\" + \"\\n\"\n",
    "              string_list[3] = \"#SBATCH --gres=gpu:1\" + \"\\n\"\n",
    "              string_list[4] = \"#SBATCH --mem=100G\" + \"\\n\"\n",
    "              string_list[8] += \" --batch_size 64 \"\n",
    "\n",
    "            elif model == \"EleutherAI/gpt-neo-1.3B\":\n",
    "              model_name = \"N2\"\n",
    "              string_list[1] = \"#SBATCH --account=def-bengioy_gpu\" + \"\\n\"\n",
    "              string_list[3] = \"#SBATCH --gres=gpu:1\" + \"\\n\"\n",
    "              string_list[4] = \"#SBATCH --mem=50G\" + \"\\n\"\n",
    "              string_list[8] += \" --batch_size 128 \"\n",
    "\n",
    "            elif model == \"distilgpt2\":\n",
    "              model_name = \"G2D\"\n",
    "            elif model == \"gpt2\":\n",
    "              model_name = \"G2\"\n",
    "            elif model == \"distilroberta-base\":\n",
    "              model_name = \"RD\"\n",
    "            elif model == \"distilbert-base-cased\":\n",
    "              model_name = \"BD\"\n",
    "\n",
    "            string_list[5] = \"#SBATCH --time=11:57:00\" + \"\\n\"\n",
    "\n",
    "            if group == \"gender_and_sex\":\n",
    "              group_name = \"g\"\n",
    "            elif group == \"race_ethnicity\":\n",
    "              group_name = \"r\"\n",
    "            elif group == \"religion\":\n",
    "              group_name = \"l\"\n",
    "            elif group == \"sexual_orientation\":\n",
    "              group_name = \"s\"\n",
    "            elif group == \"nationality\":\n",
    "              group_name = \"n\"\n",
    "\n",
    "            string_list[8] += \" --model \" + model\n",
    "\n",
    "            prompting_abbrev=\"h\"\n",
    "            string_list[8] += \" --targeted_holistic_bias \" + group\n",
    "            string_list[8] += \" --prompting \" + str(prompting)\n",
    "            string_list[7] = \"sleep \" + str(1*id) + \"\\n\"\n",
    "            file_name = \"./scripts/\" + folder_name  + \"/\" + str(prompting_abbrev) + str(seed) + \"_\" + str(model_name) + \"_h\" + str(head_knockout) + \"_\"  + str(group_name)\n",
    "            string_list[8] = string_list[8] + \" --seed \" + str(seed)\n",
    "            temp = string_list[8]\n",
    "            string_list[8] = temp + \" --head_knockout \" + str(head_knockout) + \"\\n\"\n",
    "            for h in range(1,heads_per_sh):\n",
    "              string_list[8] += temp + \" --head_knockout \" + str(head_knockout + h) + \"\\n\"\n",
    "            Path(\"./scripts/\" + folder_name).mkdir(parents=True, exist_ok=True)\n",
    "            my_file = open(file_name + \".sh\", \"w\")\n",
    "            new_file_contents = \"\".join(string_list)\n",
    "            my_file.write(new_file_contents)\n",
    "            my_file.close()\n",
    "          id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[1], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {0: [1]}\n",
      "setting 0 - 1 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 242.146414 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 235.577594 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[2], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {0: [2]}\n",
      "setting 0 - 2 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 245.586868 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 775, in urlopen\n",
      "    self._prepare_proxy(conn)\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 1044, in _prepare_proxy\n",
      "    conn.connect()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 730, in connect\n",
      "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 909, in _ssl_wrap_socket_and_match_hostname\n",
      "    ssl_sock = ssl_wrap_socket(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 469, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 513, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 517, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1104, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1382, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "TimeoutError: _ssl.c:989: The handshake operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 777, in urlopen\n",
      "    self._raise_timeout(\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[3], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {0: [3]}\n",
      "setting 0 - 3 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 230.045937 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 236.590378 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[4], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {0: [4]}\n",
      "setting 0 - 4 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 231.227791 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 325, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 286, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1314, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1166, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 538, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[5], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {0: [5]}\n",
      "setting 0 - 5 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 237.164719 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 325, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 294, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "urllib3.exceptions.ProxyError: ('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Max retries exceeded with url: /v1alpha1/comments:analyze?key=AIzaSyBS87gEEhUOnO2R085rDVE3whxE_sR2hG8 (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 671, in send\n",
      "    raise ProxyError(e, request=request)\n",
      "requests.exceptions.ProxyError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Max retries exceeded with url: /v1alpha1/comments:analyze?key=AIzaSyBS87gEEhUOnO2R085rDVE3whxE_sR2hG8 (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[6], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {0: [6]}\n",
      "setting 0 - 6 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 248.825020 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 233.525064 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[7], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {0: [7]}\n",
      "setting 0 - 7 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 272.732679 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 239.790668 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[8], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {0: [8]}\n",
      "setting 0 - 8 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 279.471094 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 238.556090 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[9], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {0: [9]}\n",
      "setting 0 - 9 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 287.407870 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 241.247580 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[10], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {0: [10]}\n",
      "setting 0 - 10 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 294.196241 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 237.112250 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[11], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {0: [11]}\n",
      "setting 0 - 11 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 292.254661 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 241.669515 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[12], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {1: [0]}\n",
      "setting 1 - 0 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 1130.293575 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 365.884191 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[13], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {1: [1]}\n",
      "setting 1 - 1 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 474.799143 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 235.326481 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[14], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {1: [2]}\n",
      "setting 1 - 2 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 265.790005 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 230.809216 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[15], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {1: [3]}\n",
      "setting 1 - 3 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 287.682355 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 236.722047 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[16], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {1: [4]}\n",
      "setting 1 - 4 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 327.225549 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 235.137949 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[17], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {1: [5]}\n",
      "setting 1 - 5 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 292.642863 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 231.731921 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[18], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {1: [6]}\n",
      "setting 1 - 6 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 240.568632 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 235.042205 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[19], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {1: [7]}\n",
      "setting 1 - 7 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 248.700753 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "urllib3.exceptions.SSLError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Max retries exceeded with url: /v1alpha1/comments:analyze?key=AIzaSyBS87gEEhUOnO2R085rDVE3whxE_sR2hG8 (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)')))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 675, in send\n",
      "    raise SSLError(e, request=request)\n",
      "requests.exceptions.SSLError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Max retries exceeded with url: /v1alpha1/comments:analyze?key=AIzaSyBS87gEEhUOnO2R085rDVE3whxE_sR2hG8 (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)')))\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[20], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {1: [8]}\n",
      "setting 1 - 8 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 248.368393 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 325, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 294, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "urllib3.exceptions.ProxyError: ('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Max retries exceeded with url: /v1alpha1/comments:analyze?key=AIzaSyBS87gEEhUOnO2R085rDVE3whxE_sR2hG8 (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 671, in send\n",
      "    raise ProxyError(e, request=request)\n",
      "requests.exceptions.ProxyError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Max retries exceeded with url: /v1alpha1/comments:analyze?key=AIzaSyBS87gEEhUOnO2R085rDVE3whxE_sR2hG8 (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[21], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {1: [9]}\n",
      "setting 1 - 9 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 271.373359 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 238.623514 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[22], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {1: [10]}\n",
      "setting 1 - 10 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 304.127322 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 239.035433 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[23], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {1: [11]}\n",
      "setting 1 - 11 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 307.803153 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 237.517009 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[24], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {2: [0]}\n",
      "setting 2 - 0 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 285.599401 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 244.100063 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[25], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {2: [1]}\n",
      "setting 2 - 1 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 274.823588 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 235.696963 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[26], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {2: [2]}\n",
      "setting 2 - 2 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 231.660281 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 233.488741 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[27], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {2: [3]}\n",
      "setting 2 - 3 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 240.540479 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 231.229968 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[28], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {2: [4]}\n",
      "setting 2 - 4 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 237.310100 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 775, in urlopen\n",
      "    self._prepare_proxy(conn)\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 1044, in _prepare_proxy\n",
      "    conn.connect()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 730, in connect\n",
      "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 909, in _ssl_wrap_socket_and_match_hostname\n",
      "    ssl_sock = ssl_wrap_socket(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 469, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 513, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 517, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1104, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1382, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "TimeoutError: _ssl.c:989: The handshake operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 777, in urlopen\n",
      "    self._raise_timeout(\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[29], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {2: [5]}\n",
      "setting 2 - 5 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 276.728561 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 231.889928 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[30], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {2: [6]}\n",
      "setting 2 - 6 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 284.477100 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 238.920539 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[31], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {2: [7]}\n",
      "setting 2 - 7 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 279.237387 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 238.076037 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[32], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {2: [8]}\n",
      "setting 2 - 8 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 285.162487 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 240.856461 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[33], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {2: [9]}\n",
      "setting 2 - 9 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 258.405094 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 775, in urlopen\n",
      "    self._prepare_proxy(conn)\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 1044, in _prepare_proxy\n",
      "    conn.connect()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 730, in connect\n",
      "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 909, in _ssl_wrap_socket_and_match_hostname\n",
      "    ssl_sock = ssl_wrap_socket(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 469, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 513, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 517, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1104, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1382, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "TimeoutError: _ssl.c:989: The handshake operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 777, in urlopen\n",
      "    self._raise_timeout(\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[34], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {2: [10]}\n",
      "setting 2 - 10 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 1274.541394 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 306.812063 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[35], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {2: [11]}\n",
      "setting 2 - 11 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 2146.647046 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 291.605365 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[36], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {3: [0]}\n",
      "setting 3 - 0 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 2283.763871 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 327.965671 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[37], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {3: [1]}\n",
      "setting 3 - 1 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 245.695461 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 775, in urlopen\n",
      "    self._prepare_proxy(conn)\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 1044, in _prepare_proxy\n",
      "    conn.connect()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 730, in connect\n",
      "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 909, in _ssl_wrap_socket_and_match_hostname\n",
      "    ssl_sock = ssl_wrap_socket(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 469, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 513, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 517, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1104, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1382, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "TimeoutError: _ssl.c:989: The handshake operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 777, in urlopen\n",
      "    self._raise_timeout(\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[38], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {3: [2]}\n",
      "setting 3 - 2 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 288.732581 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 237.398195 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[39], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {3: [3]}\n",
      "setting 3 - 3 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 281.099083 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 241.036360 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[40], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {3: [4]}\n",
      "setting 3 - 4 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 287.149149 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 237.683620 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[41], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {3: [5]}\n",
      "setting 3 - 5 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 288.960968 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 232.668889 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[42], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {3: [6]}\n",
      "setting 3 - 6 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 285.188784 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 238.024663 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[43], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {3: [7]}\n",
      "setting 3 - 7 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 287.564745 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 246.837785 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[44], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {3: [8]}\n",
      "setting 3 - 8 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 288.947968 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 240.790610 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[45], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {3: [9]}\n",
      "setting 3 - 9 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 277.767694 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 238.924808 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[46], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {3: [10]}\n",
      "setting 3 - 10 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 284.264022 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 241.970636 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[47], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {3: [11]}\n",
      "setting 3 - 11 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 283.898836 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 241.813263 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[48], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {4: [0]}\n",
      "setting 4 - 0 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 283.666055 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 242.851772 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[49], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {4: [1]}\n",
      "setting 4 - 1 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 283.729992 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 242.299533 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[50], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {4: [2]}\n",
      "setting 4 - 2 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 286.132313 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 238.381076 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[51], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {4: [3]}\n",
      "setting 4 - 3 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 286.751971 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 233.514562 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[52], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {4: [4]}\n",
      "setting 4 - 4 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 268.701721 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 237.388348 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[53], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {4: [5]}\n",
      "setting 4 - 5 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 292.894571 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 237.313738 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[54], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {4: [6]}\n",
      "setting 4 - 6 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 290.936881 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 240.644980 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[55], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {4: [7]}\n",
      "setting 4 - 7 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 289.631526 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 236.975334 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[56], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {4: [8]}\n",
      "setting 4 - 8 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 291.102668 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 236.442981 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[57], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {4: [9]}\n",
      "setting 4 - 9 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 285.545699 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 235.966689 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[58], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {4: [10]}\n",
      "setting 4 - 10 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 289.371389 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 325, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 286, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1314, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1166, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 538, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[59], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {4: [11]}\n",
      "setting 4 - 11 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 288.203200 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 238.863046 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[60], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {5: [0]}\n",
      "setting 5 - 0 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 296.871486 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 235.061153 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[61], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {5: [1]}\n",
      "setting 5 - 1 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 291.748691 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 238.458223 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[62], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {5: [2]}\n",
      "setting 5 - 2 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 290.293395 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 236.066685 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[63], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {5: [3]}\n",
      "setting 5 - 3 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 294.443270 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 239.475617 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[64], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {5: [4]}\n",
      "setting 5 - 4 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 288.193553 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 237.313355 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[65], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {5: [5]}\n",
      "setting 5 - 5 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 288.970853 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 234.260276 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[66], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {5: [6]}\n",
      "setting 5 - 6 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 271.421773 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 234.523721 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[67], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {5: [7]}\n",
      "setting 5 - 7 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 292.077322 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 235.307525 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[68], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {5: [8]}\n",
      "setting 5 - 8 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 288.869494 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 237.366223 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[69], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {5: [9]}\n",
      "setting 5 - 9 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 290.939275 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 775, in urlopen\n",
      "    self._prepare_proxy(conn)\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 1044, in _prepare_proxy\n",
      "    conn.connect()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 730, in connect\n",
      "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 909, in _ssl_wrap_socket_and_match_hostname\n",
      "    ssl_sock = ssl_wrap_socket(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 469, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 513, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 517, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1104, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1382, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "TimeoutError: _ssl.c:989: The handshake operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 777, in urlopen\n",
      "    self._raise_timeout(\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[70], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {5: [10]}\n",
      "setting 5 - 10 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 298.704564 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 237.346797 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[71], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {5: [11]}\n",
      "setting 5 - 11 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 288.714707 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 235.560043 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[72], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {6: [0]}\n",
      "setting 6 - 0 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 285.998964 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 236.341513 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[73], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {6: [1]}\n",
      "setting 6 - 1 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 291.285822 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 235.261674 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[74], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {6: [2]}\n",
      "setting 6 - 2 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 246.134799 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 775, in urlopen\n",
      "    self._prepare_proxy(conn)\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 1044, in _prepare_proxy\n",
      "    conn.connect()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 730, in connect\n",
      "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 909, in _ssl_wrap_socket_and_match_hostname\n",
      "    ssl_sock = ssl_wrap_socket(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 469, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 513, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 517, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1104, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1382, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "TimeoutError: _ssl.c:989: The handshake operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 777, in urlopen\n",
      "    self._raise_timeout(\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[75], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {6: [3]}\n",
      "setting 6 - 3 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 266.902263 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 243.399128 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[76], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {6: [4]}\n",
      "setting 6 - 4 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 284.739832 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 237.091094 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[77], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {6: [5]}\n",
      "setting 6 - 5 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 281.527616 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 238.387685 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[78], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {6: [6]}\n",
      "setting 6 - 6 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 283.551260 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 237.485355 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[79], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {6: [7]}\n",
      "setting 6 - 7 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 284.213632 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 240.437969 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[80], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {6: [8]}\n",
      "setting 6 - 8 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 285.107739 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 241.249786 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[81], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {6: [9]}\n",
      "setting 6 - 9 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 286.219372 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 243.631581 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[82], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {6: [10]}\n",
      "setting 6 - 10 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 268.600370 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 243.567543 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[83], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {6: [11]}\n",
      "setting 6 - 11 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 287.990562 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 240.862968 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[84], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {7: [0]}\n",
      "setting 7 - 0 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 283.939458 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 241.988006 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[85], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {7: [1]}\n",
      "setting 7 - 1 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 285.022057 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 240.540437 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[86], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {7: [2]}\n",
      "setting 7 - 2 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 284.524154 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 241.774706 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[87], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {7: [3]}\n",
      "setting 7 - 3 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 287.458630 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 240.383139 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[88], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {7: [4]}\n",
      "setting 7 - 4 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 296.455591 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 241.634787 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[89], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {7: [5]}\n",
      "setting 7 - 5 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 276.418128 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 239.169471 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[90], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {7: [6]}\n",
      "setting 7 - 6 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 284.218108 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 234.584994 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[91], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {7: [7]}\n",
      "setting 7 - 7 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 287.297369 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 237.144234 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[92], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {7: [8]}\n",
      "setting 7 - 8 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 283.622878 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 241.766205 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[93], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {7: [9]}\n",
      "setting 7 - 9 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 283.734633 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 235.800402 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[94], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {7: [10]}\n",
      "setting 7 - 10 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 291.059059 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 236.739196 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[95], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {7: [11]}\n",
      "setting 7 - 11 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 283.011869 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 241.453182 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[96], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {8: [0]}\n",
      "setting 8 - 0 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 272.878067 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 235.342124 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[97], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {8: [1]}\n",
      "setting 8 - 1 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 289.421278 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 232.838268 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[98], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {8: [2]}\n",
      "setting 8 - 2 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 298.028871 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 234.494365 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[99], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {8: [3]}\n",
      "setting 8 - 3 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 293.830088 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 232.836313 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[100], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {8: [4]}\n",
      "setting 8 - 4 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 261.955439 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 226.575769 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[101], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {8: [5]}\n",
      "setting 8 - 5 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 248.321704 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 228.671942 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[102], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {8: [6]}\n",
      "setting 8 - 6 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 266.294482 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 231.133118 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[103], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {8: [7]}\n",
      "setting 8 - 7 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 532.400973 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 293.730738 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[104], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {8: [8]}\n",
      "setting 8 - 8 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 2002.552518 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 300.078168 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[105], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {8: [9]}\n",
      "setting 8 - 9 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 2219.987915 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 327.741232 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[106], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {8: [10]}\n",
      "setting 8 - 10 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 2384.696900 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 298.076675 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[107], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {8: [11]}\n",
      "setting 8 - 11 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 386.740754 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 231.249058 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[108], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {9: [0]}\n",
      "setting 9 - 0 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 322.148975 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 243.035025 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[109], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {9: [1]}\n",
      "setting 9 - 1 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 2153.898659 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 298.132832 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[110], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {9: [2]}\n",
      "setting 9 - 2 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 502.425454 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 230.127273 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[111], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {9: [3]}\n",
      "setting 9 - 3 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 245.213406 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 231.059199 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[112], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {9: [4]}\n",
      "setting 9 - 4 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 235.027948 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 223.286658 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[113], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {9: [5]}\n",
      "setting 9 - 5 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 233.288926 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 229.431136 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[114], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {9: [6]}\n",
      "setting 9 - 6 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 231.517005 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 228.813842 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[115], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {9: [7]}\n",
      "setting 9 - 7 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 230.729354 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 775, in urlopen\n",
      "    self._prepare_proxy(conn)\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 1044, in _prepare_proxy\n",
      "    conn.connect()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 730, in connect\n",
      "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 909, in _ssl_wrap_socket_and_match_hostname\n",
      "    ssl_sock = ssl_wrap_socket(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 469, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 513, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 517, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1104, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1382, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "TimeoutError: _ssl.c:989: The handshake operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 777, in urlopen\n",
      "    self._raise_timeout(\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[116], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {9: [8]}\n",
      "setting 9 - 8 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 251.646367 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 227.944948 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[117], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {9: [9]}\n",
      "setting 9 - 9 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 237.269333 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 325, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 286, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1314, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1166, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 538, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[118], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {9: [10]}\n",
      "setting 9 - 10 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 261.567498 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 235.244777 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[119], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {9: [11]}\n",
      "setting 9 - 11 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 275.521915 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 233.242948 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[120], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {10: [0]}\n",
      "setting 10 - 0 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 281.811357 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 775, in urlopen\n",
      "    self._prepare_proxy(conn)\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 1044, in _prepare_proxy\n",
      "    conn.connect()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 730, in connect\n",
      "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 909, in _ssl_wrap_socket_and_match_hostname\n",
      "    ssl_sock = ssl_wrap_socket(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 469, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 513, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 517, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1104, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1382, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "TimeoutError: _ssl.c:989: The handshake operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 777, in urlopen\n",
      "    self._raise_timeout(\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[121], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {10: [1]}\n",
      "setting 10 - 1 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 327.708361 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 234.024494 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[122], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {10: [2]}\n",
      "setting 10 - 2 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 285.690660 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 240.786744 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[123], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {10: [3]}\n",
      "setting 10 - 3 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 284.075357 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 242.150945 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[124], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {10: [4]}\n",
      "setting 10 - 4 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 282.026665 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 775, in urlopen\n",
      "    self._prepare_proxy(conn)\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 1044, in _prepare_proxy\n",
      "    conn.connect()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 730, in connect\n",
      "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 909, in _ssl_wrap_socket_and_match_hostname\n",
      "    ssl_sock = ssl_wrap_socket(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 469, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 513, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 517, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1104, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1382, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "TimeoutError: _ssl.c:989: The handshake operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 777, in urlopen\n",
      "    self._raise_timeout(\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[125], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {10: [5]}\n",
      "setting 10 - 5 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 309.628363 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 245.813224 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[126], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {10: [6]}\n",
      "setting 10 - 6 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 289.225500 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 775, in urlopen\n",
      "    self._prepare_proxy(conn)\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 1044, in _prepare_proxy\n",
      "    conn.connect()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 730, in connect\n",
      "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 909, in _ssl_wrap_socket_and_match_hostname\n",
      "    ssl_sock = ssl_wrap_socket(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 469, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/ssl_.py\", line 513, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 517, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1104, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1382, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "TimeoutError: _ssl.c:989: The handshake operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 777, in urlopen\n",
      "    self._raise_timeout(\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[127], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {10: [7]}\n",
      "setting 10 - 7 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 300.162229 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 237.699526 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[128], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {10: [8]}\n",
      "setting 10 - 8 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 295.498918 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 238.401004 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[129], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {10: [9]}\n",
      "setting 10 - 9 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 291.709929 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 243.362889 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[130], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {10: [10]}\n",
      "setting 10 - 10 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 285.545076 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 239.788950 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[131], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {10: [11]}\n",
      "setting 10 - 11 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 286.153523 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 242.489837 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[132], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {11: [0]}\n",
      "setting 11 - 0 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 283.364825 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 244.120685 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[133], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {11: [1]}\n",
      "setting 11 - 1 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 293.835329 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 258.118737 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[134], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {11: [2]}\n",
      "setting 11 - 2 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 291.591270 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 262.034759 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[135], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {11: [3]}\n",
      "setting 11 - 3 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 289.172391 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 261.290792 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[136], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {11: [4]}\n",
      "setting 11 - 4 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 305.671907 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 257.612844 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[137], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {11: [5]}\n",
      "setting 11 - 5 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 318.958707 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 264.955071 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[138], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {11: [6]}\n",
      "setting 11 - 6 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 315.228623 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 261.379396 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[139], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {11: [7]}\n",
      "setting 11 - 7 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 311.619492 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 325, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/http/client.py\", line 286, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1314, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/ssl.py\", line 1166, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 538, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 373, in <module>\n",
      "    generations, toxicity_scores = gen_prompt(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 105, in wrapper\n",
      "    result = func(*args, **kwargs) \n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 275, in gen_prompt\n",
      "    toxicity_scores = analyze_toxicity(outputs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/Documents/!Phd/llm/shiyicode/maskattentionmap.py\", line 184, in analyze_toxicity\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shiyijiang/anaconda3/envs/learning/lib/python3.11/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='commentanalyzer.googleapis.com', port=443): Read timed out. (read timeout=5)\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[140], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {11: [8]}\n",
      "setting 11 - 8 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 339.497154 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 259.532334 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[141], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {11: [9]}\n",
      "setting 11 - 9 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 305.532055 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 258.055637 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[142], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {11: [10]}\n",
      "setting 11 - 10 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 322.447002 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 262.865863 seconds\n",
      "Namespace(seed=1, model='EleutherAI/gpt-neo-125M', banheads=[143], splits=['valid'], targeted_bias='gender_and_sex', stride=512, batch_size=128, max_continuation_length=40, max_prompt_length=22)\n",
      "Currently used model: EleutherAI/gpt-neo-125M\n",
      "max_length: 2048\n",
      "num of layers: 12\n",
      "num of heads: 12\n",
      "head dim: 64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned heads: {11: [11]}\n",
      "setting 11 - 11 to zeros...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 2048). Running this sequence through the model will result in indexing errors\n",
      "function compute_ppl exe time: 308.615553 seconds\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "function gen_prompt exe time: 253.570441 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "!python maskattentionmap.py --seed 1 --model EleutherAI/gpt-neo-125M --splits valid --targeted_bias gender_and_sex\n",
    "\n",
    "for headid in range(12*12):\n",
    "    !python maskattentionmap.py --seed 1 --model EleutherAI/gpt-neo-125M --banheads {headid} --splits valid --targeted_bias gender_and_sex\n",
    "\n",
    "# !python maskattentionmap.py --seed 1 --model EleutherAI/gpt-neo-125M --splits valid --targeted_bias gender_and_sex\n",
    "# !python maskattentionmap.py --seed 1 --model EleutherAI/gpt-neo-125M --banheads 0 --splits valid --targeted_bias gender_and_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current model: EleutherAI_gpt-neo-125M with banheads noban, ppl: 37.105, bias: 0.8963049323119895\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 0, ppl: 41.532, bias: 0.9723149261481883\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 1, ppl: 47.325, bias: 0.9754159944717719\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 2\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 3, ppl: 38.901, bias: 0.7780039029570487\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 4\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 5\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 6, ppl: 31942914.0, bias: 0.7707042752170319\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 7, ppl: 53.215, bias: 0.8491337795160099\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 8, ppl: 132045360.0, bias: 0.7148991773752472\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 9, ppl: 38.525, bias: 1.0910787104077448\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 10, ppl: 39.762, bias: 0.9547448529469861\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 11, ppl: 50.323, bias: 0.9120227591400775\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 12, ppl: 124.764, bias: 0.6609439396613863\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 13, ppl: 37.599, bias: 1.0290510409722124\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 14, ppl: 37.652, bias: 0.8498841088010705\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 15, ppl: 37.135, bias: 0.94393721575098\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 16, ppl: 37.869, bias: 0.9529583406258565\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 17, ppl: 37.871, bias: 0.9920406854469244\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 18, ppl: 64.856, bias: 0.8855889928653244\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 19\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 20\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 21, ppl: 37.788, bias: 0.9727567861100133\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 22, ppl: 37.491, bias: 1.0071479617084815\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 23, ppl: 38.163, bias: 0.9960928917326459\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 24, ppl: 38.357, bias: 0.9262781225103632\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 25, ppl: 37.573, bias: 1.0228622067899333\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 26, ppl: 47.238, bias: 0.8907822749628047\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 27, ppl: 37.38, bias: 0.9468212409185977\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 28\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 29, ppl: 38.719, bias: 0.8645687170858779\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 30, ppl: 38.327, bias: 1.1160204942053935\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 31, ppl: 37.799, bias: 0.9632097821527421\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 32, ppl: 41.694, bias: 0.8902874453273673\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 33\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 34, ppl: 37.224, bias: 0.9776770695990407\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 35, ppl: 37.869, bias: 0.9828468115949933\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 36, ppl: 37.423, bias: 0.9407851741416914\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 37\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 38, ppl: 37.484, bias: 0.9281102509728508\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 39, ppl: 37.287, bias: 0.9582276417208621\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 40, ppl: 37.754, bias: 0.9482636672093914\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 41, ppl: 37.57, bias: 0.9568007532102932\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 42, ppl: 37.818, bias: 0.9577543991048871\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 43, ppl: 37.719, bias: 1.030258075218155\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 44, ppl: 37.316, bias: 1.1088823977415843\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 45, ppl: 37.597, bias: 0.9738036883245346\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 46, ppl: 37.159, bias: 1.0717221949909421\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 47, ppl: 37.934, bias: 1.169350530108362\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 48, ppl: 37.985, bias: 0.897418711972616\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 49, ppl: 37.457, bias: 0.9762327909036871\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 50, ppl: 37.632, bias: 0.9931653124768856\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 51, ppl: 38.102, bias: 1.0029649811800643\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 52, ppl: 38.087, bias: 0.8376730140288209\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 53, ppl: 37.865, bias: 0.9830062827824604\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 54, ppl: 37.727, bias: 1.0502750529007328\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 55, ppl: 37.397, bias: 0.9150472949861783\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 56, ppl: 37.568, bias: 0.922720154803788\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 57, ppl: 38.856, bias: 0.9558833945709816\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 58\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 59, ppl: 37.72, bias: 0.9039758790433012\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 60, ppl: 38.249, bias: 0.9212031402301055\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 61, ppl: 38.559, bias: 0.8826841310590373\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 62, ppl: 37.802, bias: 0.967602794481526\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 63, ppl: 38.984, bias: 1.0017849020934495\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 64, ppl: 38.086, bias: 1.068465298735285\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 65, ppl: 37.68, bias: 0.8778085772073451\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 66, ppl: 38.07, bias: 0.9162587957535325\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 67, ppl: 44.875, bias: 0.9880767424745469\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 68, ppl: 44.144, bias: 0.8969996338845726\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 69\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 70, ppl: 37.671, bias: 0.8830436771885313\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 71, ppl: 37.865, bias: 0.9827402469182311\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 72, ppl: 38.282, bias: 0.9169649559333415\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 73, ppl: 37.79, bias: 0.985562242235853\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 74\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 75, ppl: 37.708, bias: 0.9439708405392129\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 76, ppl: 78.204, bias: 0.7868466233668479\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 77, ppl: 37.217, bias: 0.8909601410899334\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 78, ppl: 23804.957, bias: 1.034058388132864\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 79, ppl: 38.208, bias: 0.9446750411153408\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 80, ppl: 37.517, bias: 1.0067229590755846\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 81, ppl: 38.16, bias: 0.9480864411880763\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 82, ppl: 38.248, bias: 0.8736449720510129\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 83, ppl: 37.474, bias: 0.9507368593248104\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 84, ppl: 38.108, bias: 1.0226820740834157\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 85, ppl: 38.758, bias: 1.0595092264933919\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 86, ppl: 37.207, bias: 0.950158100729356\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 87, ppl: 37.93, bias: 0.8912534673975503\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 88, ppl: 37.651, bias: 0.9002365245203806\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 89, ppl: 37.657, bias: 1.0180028322789525\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 90, ppl: 37.986, bias: 1.1107880263401269\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 91, ppl: 37.702, bias: 1.0843787546893195\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 92, ppl: 37.244, bias: 1.1804572961852788\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 93, ppl: 37.581, bias: 0.9293571233671196\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 94, ppl: 37.427, bias: 0.9172896809088851\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 95, ppl: 38.985, bias: 1.0079046776736948\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 96, ppl: 37.377, bias: 0.967842930373732\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 97, ppl: 37.218, bias: 0.9817038780344205\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 98, ppl: 37.39, bias: 0.9742706775125781\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 99, ppl: 37.547, bias: 0.8844522577172185\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 100, ppl: 38.817, bias: 1.0571786213924408\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 101, ppl: 39.965, bias: 0.9716055037516882\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 102, ppl: 38.423, bias: 0.9256882267923379\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 103, ppl: 37.285, bias: 0.8503135699834322\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 104, ppl: 37.217, bias: 0.9403619804259469\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 105, ppl: 37.981, bias: 0.9317301142040431\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 106, ppl: 37.281, bias: 1.0219920163065093\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 107, ppl: 37.224, bias: 0.9127711381637762\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 108, ppl: 38.209, bias: 0.8964683398331194\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 109, ppl: 38.603, bias: 0.949216135570891\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 110, ppl: 37.496, bias: 0.8384889361109683\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 111, ppl: 37.742, bias: 0.9019187690376564\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 112, ppl: 37.907, bias: 0.8803311840256959\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 113, ppl: 38.214, bias: 0.9626533497499383\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 114, ppl: 37.366, bias: 1.0028497681393487\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 115\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 116, ppl: 37.93, bias: 0.8492567134797924\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 117\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 118, ppl: 38.289, bias: 0.9458251021354538\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 119, ppl: 37.812, bias: 0.926477849481464\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 120\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 121, ppl: 37.543, bias: 1.0268048921647026\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 122, ppl: 37.278, bias: 0.9711058709411232\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 123, ppl: 37.437, bias: 0.8976978727893528\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 124\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 125, ppl: 37.492, bias: 0.916959259115234\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 126\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 127, ppl: 37.463, bias: 0.9317413005643941\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 128, ppl: 45.564, bias: 1.0012519134380025\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 129, ppl: 37.263, bias: 1.0177752697821227\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 130, ppl: 37.415, bias: 0.9618184265117341\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 131, ppl: 37.856, bias: 0.9995885557938902\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 132, ppl: 37.366, bias: 1.120377613537109\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 133, ppl: 37.464, bias: 0.9052183795318223\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 134, ppl: 39.184, bias: 0.9029614451338026\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 135, ppl: 37.277, bias: 0.9569092038307557\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 136, ppl: 69.487, bias: 0.6136540509811204\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 137, ppl: 37.333, bias: 0.9896448619164773\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 138, ppl: 37.95, bias: 1.0055534938830903\n",
      "ERROR! current model: EleutherAI_gpt-neo-125M with banheads 139\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 140, ppl: 37.217, bias: 0.9883569981290761\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 141, ppl: 37.586, bias: 0.9907145633407772\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 142, ppl: 37.692, bias: 0.992735467376398\n",
      "current model: EleutherAI_gpt-neo-125M with banheads 143, ppl: 37.251, bias: 1.0477566069167572\n"
     ]
    }
   ],
   "source": [
    "# head contribution \n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "df_all_seeds = pd.DataFrame() \n",
    "\n",
    "# this files has the information about the groups that are targeted in the validation data prompts (for example, different religions, genders, etc.)\n",
    "groups_valid = {}\n",
    "groups_valid[\"axis\"] =  json.load(open(\"./prompts/holistic/social_biases_valid_groups.json\", \"r\"))[\"axis\"]\n",
    "groups_valid[\"bucket\"] = json.load(open(\"./prompts/holistic/social_biases_valid_groups.json\", \"r\"))[\"bucket\"]\n",
    "groups_valid = pd.DataFrame.from_dict(groups_valid)\n",
    "\n",
    "groups_test = {}\n",
    "groups_test[\"axis\"] =  json.load(open(\"./prompts/holistic/social_biases_test_groups.json\", \"r\"))[\"axis\"]\n",
    "groups_test[\"bucket\"] = json.load(open(\"./prompts/holistic/social_biases_test_groups.json\", \"r\"))[\"bucket\"]\n",
    "groups_test = pd.DataFrame.from_dict(groups_test)\n",
    "\n",
    "\n",
    "ppls, biass = [], [] \n",
    "\n",
    "seeds = [\"1\"]\n",
    "model_list = [\"EleutherAI/gpt-neo-125M\"]\n",
    "\n",
    "groups = [\"gender_and_sex\"] # \"nationality\", \"race_ethnicity\", \"religion\", \"sexual_orientation\"\n",
    "\n",
    "for seed in seeds:\n",
    "    for model_name in model_list:\n",
    "        model_configs = json.load(open(\"./models_config.json\", \"r\"))\n",
    "        num_heads, num_layers = model_configs[model_name][\"num_heads\"], model_configs[model_name][\"num_layers\"] \n",
    "        head_dim, max_length = model_configs[model_name][\"head_dim\"], model_configs[model_name][\"max_length\"] \n",
    "        for banhead in [\"noban\"] + list(range(num_layers*num_heads)):\n",
    "            for split in [\"valid\"]:\n",
    "                for group in groups: \n",
    "                    try: \n",
    "                        model_name = model_name.replace(\"/\", \"_\")\n",
    "                        gen_file = f\"output/{seed}/{model_name}_{banhead}/{split}_{group}/generations.json\"\n",
    "                        gen_dict = json.load(open(gen_file, \"r\")) \n",
    "                        ppl_file = f\"output/{seed}/{model_name}_{banhead}/ppl.json\" \n",
    "                        ppl_dict = json.load(open(ppl_file, \"r\")) \n",
    "                        ppl = ppl_dict[\"valid\"]\n",
    "                        if split == \"valid\":\n",
    "                            groups_split = groups_valid\n",
    "                        elif split == \"test\":\n",
    "                            groups_split = groups_test\n",
    "                        \n",
    "                        gen_groups = list(groups_split[\"bucket\"][groups_split[\"axis\"] == group]) \n",
    "                        idx = 0\n",
    "                        for item in gen_dict[\"generations\"]:\n",
    "                            item[\"subgroup\"] = gen_groups[idx]\n",
    "                            idx += 1  \n",
    "                        df = pd.DataFrame(gen_dict[\"generations\"]) \n",
    "                        # df = df[df[\"subgroup\"] != \"(none)\"]\n",
    "                        bias = 0\n",
    "                        toxicity_group = df[\"toxicity_score\"].mean()\n",
    "                        for subgroup in df[\"subgroup\"].unique():\n",
    "                            # print(df[\"toxicity_score\"][df[\"subgroup\"] == subgroup])\n",
    "                            toxicity_subgroup = df[\"toxicity_score\"][df[\"subgroup\"] == subgroup].mean()\n",
    "                            bias += abs(toxicity_group-toxicity_subgroup)\n",
    "                        \n",
    "                        print(f\"current model: {model_name} with banheads {banhead}, ppl: {ppl}, bias: {bias}\")\n",
    "                        ppls.append(ppl)\n",
    "                        biass.append(bias)\n",
    "                    except Exception as e:\n",
    "                        print(f\"ERROR! current model: {model_name} with banheads {banhead}\")\n",
    "                        ppls.append(-1)\n",
    "                        biass.append(-1)\n",
    "                    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value greater than 100: (these values will be set to ppl of noban)\n",
      "7 31942914.0\n",
      "9 132045360.0\n",
      "13 124.764\n",
      "79 23804.957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1748a1110>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "xs = list(range(len(ppls)))\n",
    "plot_ppls = copy.deepcopy(ppls)\n",
    "\n",
    "print(\"The value greater than 100: (these values will be set to ppl of noban)\") \n",
    "for i in range(len(ppls)):\n",
    "    if ppls[i] > 100:\n",
    "        print(i, ppls[i])   \n",
    "        plot_ppls[i] = ppls[0]\n",
    "    elif ppls[i] == -1: # TODO \n",
    "        plot_ppls[i] = ppls[0]\n",
    "    elif ppls[i] < ppls[0]:\n",
    "        print(f\"reduced idx {i}\")\n",
    "\n",
    "plt.plot(xs, plot_ppls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1750bc590>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "xs = list(range(len(biass)))\n",
    "plot_biass = copy.deepcopy(biass)\n",
    "# print(\"The value greater than 100:\") \n",
    "for i in range(len(biass)):\n",
    "    if biass[i] == -1: \n",
    "        plot_biass[i] = biass[0]\n",
    "\n",
    "plt.plot(xs, plot_biass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import getgroups\n",
    "from pathlib import Path\n",
    "\n",
    "id = 0\n",
    "folder_name = \"experiment_2\"\n",
    "for group in [\"gender_and_sex\",\"race_ethnicity\", \"religion\", \"sexual_orientation\", \"nationality\", \"gender_and_sex\"]:\n",
    "  for model in [\"gpt2\",\"distilgpt2\",\"EleutherAI/gpt-neo-125M\",\"EleutherAI/gpt-neo-1.3B\",\"EleutherAI/gpt-j-6B\",\"meta-llama/Llama-2-7b-chat-hf\"]:\n",
    "    for pruned_heads_ratio in np.linspace(0,0.2,11,endpoint=True):\n",
    "      for prompting in [\"holistic\"]:\n",
    "        for method in [\"FASP\", \"bias_only\", \"ppl_only\",\"random_structured\", \"mask_gradient_l2_structured\", \"magnitude_l2_structured\"]:\n",
    "          if method == \"FASP\":\n",
    "            gammas = [\"0.2\", \"0.3\",\"0.4\",\"0.5\",\"0.6\", \"0.7\"]\n",
    "          else:\n",
    "            gammas = [\"None\"]\n",
    "          for gamma in gammas:\n",
    "              for seed in range(1,4):\n",
    "                my_file = open(\"./scripts/sample.sh\")\n",
    "                string_list = my_file.readlines()\n",
    "\n",
    "                my_file.close()\n",
    "\n",
    "                string_list[1] = \"#SBATCH --account=rrg-bengioy-ad_gpu\" + \"\\n\"\n",
    "                string_list[2] = \"#SBATCH --cpus-per-task=4\" + \"\\n\"\n",
    "                string_list[3] = \"#SBATCH --gres=gpu:1\" + \"\\n\"\n",
    "                string_list[4] = \"#SBATCH --mem=100G\" + \"\\n\"\n",
    "                string_list[8] = \"python main.py \"\n",
    "                string_list[5] = \"#SBATCH --time=2:57:00\" + \"\\n\"\n",
    "\n",
    "                if model == \"EleutherAI/gpt-neo-125M\":\n",
    "                  model_name = \"N1\"\n",
    "                elif model == \"EleutherAI/gpt-neo-1.3B\":\n",
    "                  model_name = \"N2\"\n",
    "                  string_list[8] += \" --batch_size 32 \"\n",
    "                elif model == \"EleutherAI/gpt-j-6B\":\n",
    "                  model_name = \"NJ\"\n",
    "                  string_list[5] = \"#SBATCH --time=11:57:00\" + \"\\n\"\n",
    "                  string_list[8] += \" --batch_size 8 \"\n",
    "                elif model == \"meta-llama/Llama-2-7b-chat-hf\":\n",
    "                  model_name = \"L7\"\n",
    "                  string_list[5] = \"#SBATCH --time=11:57:00\" + \"\\n\"\n",
    "                  string_list[8] += \" --batch_size 8 \"\n",
    "                elif model == \"distilgpt2\":\n",
    "                  model_name = \"G2D\"\n",
    "                elif model == \"gpt2\":\n",
    "                  model_name = \"G2\"\n",
    "\n",
    "                if group == \"gender_and_sex\":\n",
    "                  group_name = \"g\"\n",
    "                elif group == \"race_ethnicity\":\n",
    "                  group_name = \"r\"\n",
    "                elif group == \"religion\":\n",
    "                  group_name = \"l\"\n",
    "                elif group == \"sexual_orientation\":\n",
    "                  group_name = \"s\"\n",
    "                elif group == \"nationality\":\n",
    "                  group_name = \"n\"\n",
    "\n",
    "                string_list[8] += \" --model \" + model\n",
    "                string_list[8] += \" --method \" + str(method)\n",
    "                string_list[8] += \" --pruned_heads_ratio \" + str(pruned_heads_ratio)\n",
    "                if gamma != \"None\":\n",
    "                  string_list[8] += \" --gamma \" + str(gamma)\n",
    "\n",
    "                if prompting == \"holistic\":\n",
    "                  prompting_abbrev=\"h\"\n",
    "                  string_list[8] += \" --targeted_holistic_bias \" + group\n",
    "\n",
    "                if method == \"random_structured\":\n",
    "                  method_name = \"rn\"\n",
    "                elif method == \"FASP\":\n",
    "                  method_name = \"f\"\n",
    "                elif method == \"magnitude_l2_structured\":\n",
    "                  method_name = \"m2\"\n",
    "                elif method == \"mask_gradient_l2_structured\":\n",
    "                  method_name = \"mg2\"\n",
    "                elif method == \"bias_only\":\n",
    "                  method_name = \"bo\"\n",
    "                elif method == \"ppl_only\":\n",
    "                  method_name = \"po\"\n",
    "\n",
    "                string_list[8] += \" --prompting \" + str(prompting)\n",
    "                string_list[7] = \"sleep \" + str(1*id) + \"\\n\"\n",
    "                file_name = \"./scripts/\" + folder_name  + \"/\" + str(prompting_abbrev) + str(seed) + \"_\" + str(model_name) + \"_\" + str(method_name) + \"_\"  + str(pruned_heads_ratio) + \"_\"  + str(group_name)\n",
    "\n",
    "                if method == \"FASP\":\n",
    "                  file_name += \"_\" + str(gamma)\n",
    "\n",
    "                string_list[8] += \" --seed \" + str(seed) + \" \\n\"\n",
    "                Path(\"./scripts/\" + folder_name).mkdir(parents=True, exist_ok=True)\n",
    "                my_file = open(file_name + \".sh\", \"w\")\n",
    "                new_file_contents = \"\".join(string_list)\n",
    "                my_file.write(new_file_contents)\n",
    "                my_file.close()\n",
    "              id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dfc429b3c5fa84252a7a241583d20cb1b2096832c2ca87b0375b3b29de19564d"
  },
  "kernelspec": {
   "display_name": "Python 3.11.11 ('learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
